{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 3\n",
    "## Efficiently finding optimal policies in MABs\n",
    "\n",
    "In this assignment, we will work with Multi Armed Bandit environments, and try to find the best policies using different strategies to minimize the total regret.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, the regret $\\mathcal{T}$ is given by,\n",
    "\n",
    "$$\\mathcal{T}=\\sum  _{e=1} ^{E} \\mathbb{E} \\left[ v_* - q_* \\left( A_e \\right) \\right]$$\n",
    "\n",
    "We can only calculate it when we have the $v_*$ and $q_*$ functions known beforehand. Since we are making the MDPs from scratch, that's not an issue for us right now.\n",
    "\n",
    "But remember, in real-life problems, these functions are not known. Hence we must be aware of multiple policy finding strategies and try the one which gives best results fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary stuff\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# if you want to use envs from Gym, import it\n",
    "# import gym, gym_bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple **2-armed Bernoulli** bandit.\n",
    "\n",
    "If you want a cleaner code, you can implement Bandits using `class` in Python.\n",
    "\n",
    "We have included sample code for this in `bandits.py` which you can take/import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the underlying probability distribution\n",
    "\n",
    "probs = np.random.random(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our MDP is a function which takes an action and returns a reward\n",
    "\n",
    "def mab_2_env(action):\n",
    "    gen = np.random.random()\n",
    "\n",
    "    # for bernoulli bandits, the reward is 1 if the random number is less than the probability of success, else 0\n",
    "    return gen < probs[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a template for testing different strategies.\n",
    "\n",
    "Feel free to modify this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy function takes in the environment function, number of actions, and a selector function\n",
    "# it also takes in the number of episodes to run the strategy for (higher episodes = more accurate Q values)\n",
    "\n",
    "def strategy(env, n_actions, selector, n_episodes = 1000):\n",
    "    \n",
    "    # initialize Q and N to 0s\n",
    "    Q = np.zeros(n_actions)\n",
    "    N = np.zeros(n_actions)\n",
    "\n",
    "    # loop for n_episodes\n",
    "    for e in tqdm(range(n_episodes)):\n",
    "        \n",
    "        # selector function takes in current Q and returns an action\n",
    "        # modify the selector function according to the strategy\n",
    "        action = selector(Q)\n",
    "\n",
    "        # get the reward from the environment\n",
    "        reward = env(action)\n",
    "\n",
    "        # update N and Q\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action])/N[action]\n",
    "\n",
    "    # return the best action\n",
    "    return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the simplest selector using pure-exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector returns a random action\n",
    "selector = lambda Q : np.random.randint(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479a7dee46594cae85abc84a0ff30cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy(mab_2_env, 2, selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it returns the optimal action. Let's check if that's indeed true.\n",
    "\n",
    "We can do that by revealing the actual `probs` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58137579, 0.24775398])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our pure exploration strategy does indeed return the optimal action for this Bernoulli bandit. \n",
    "\n",
    "You can try generating new bandits with different `probs` and try out the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
